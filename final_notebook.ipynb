{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import wrangle as wr\n",
    "import preprocessing_permits as pr\n",
    "import explore as ex\n",
    "import model as mo\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about the data for this project:\n",
    "The initial project was performed using a HUD dataset that we felt as a team did not meet the requirements to answer the question for the capstone.  All of the initial exploration, modeling, feature engineering, labeling and preprocessing was done initially on that data set.  Upon completion we looked for a larger data set to continue working on our project.  The work for that dataset, which we believe is more complete, is below.   Our initial project can be seen [here](http://localhost:8888/notebooks/mvp_notebook.ipynb)\n",
    "\n",
    "## Why we looked for new data:\n",
    "For our project we wanted to explore and model multifamily unit construction by city over time.  The dataset we initially found was a list of Housing and Urban Development (HUD) mortgages over the past 15 years.  As we explored the data we came upon four problems that we thought we needed to overcome:\n",
    "    1. Not all construction that happens uses a HUD backed mortgage (so we did not have a complete view of construction behavior)\n",
    "    2. 80% of the data set was for refinanced mortgages not new construction.  There are many reasons to refinance a loan and we could not unpack which of these refinances indicated refinancing a construction loan. [read more here] (https://www.reonomy.com/blog/post/commercial-loan-refinance)\n",
    "    3. After reshaping that dataframe to make a single observation a `city-state-year` from a single mortgage we were left with a small observation size of cities that provided us with continuous information for the time period (we do know that we could have put in zeros for `city-state-year` for missing year values, but:)\n",
    "    4. Too many zeros, are continuous variables ended up behaving like discrete variables because we had so many instances of just one or zero mortgages for a `city-state-year` observation.\n",
    "    \n",
    "## Our new data:   Building Permit Data\n",
    "We found department of commerce data going back to 1997 for construction permits and valuation of construction for metropolitan areas.  Our **acquisition**, **preparation**, **exploration**, **preprocessing**, **clustering**, **modeling**, **evaluations**, **predictions** and **conclusions** are below!\n",
    "\n",
    "[United States Census Bureau Building Permits Survey](https://www.census.gov/construction/bps/)\n",
    "\n",
    "[ASCII files by State, Metropolitan Statistical Area (MSA), County or Place](https://www2.census.gov/econ/bps/)\n",
    "\n",
    "[MSA Folder](https://www2.census.gov/econ/bps/Metro/)\n",
    "\n",
    "[ASCII MSA Documentation](https://www2.census.gov/econ/bps/Documentation/msaasc.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_building_permit_columns(df):\n",
    "    \"\"\"\n",
    "    Docstring\n",
    "    \"\"\"\n",
    "    \n",
    "    # rename columns inplace\n",
    "    df.rename(\n",
    "        columns={\n",
    "            \"Date\": \"survey_date\",\n",
    "            \"Code\": \"csa_code\",\n",
    "            \"Code.1\": \"cbsa_code\",\n",
    "            \"Unnamed: 3\": \"moncov\",\n",
    "            \"Name\": \"cbsa_name\",\n",
    "            \"Bldgs\": \"one_unit_bldgs_est\",\n",
    "            \"Units\": \"one_unit_units_est\",\n",
    "            \"Value\": \"one_unit_value_est\",\n",
    "            \"Bldgs.1\": \"two_units_bldgs_est\",\n",
    "            \"Units.1\": \"two_units_units_est\",\n",
    "            \"Value.1\": \"two_units_value_est\",\n",
    "            \"Bldgs.2\": \"three_to_four_units_bldgs_est\",\n",
    "            \"Units.2\": \"three_to_four_units_units_est\",\n",
    "            \"Value.2\": \"three_to_four_units_value_est\",\n",
    "            \"Bldgs.3\": \"five_or_more_units_bldgs_est\",\n",
    "            \"Units.3\": \"five_or_more_units_units_est\",\n",
    "            \"Value.3\": \"five_or_more_units_value_est\",\n",
    "            \"Bldgs.4\": \"one_unit_bldgs_rep\",\n",
    "            \"Units.4\": \"one_unit_units_rep\",\n",
    "            \"Value.4\": \"one_unit_value_rep\",\n",
    "            \"Bldgs.5\": \"two_units_bldgs_rep\",\n",
    "            \"Units.5\": \"two_units_units_rep\",\n",
    "            \"Value.5\": \"two_units_value_rep\",\n",
    "            \"      Bldgs\": \"three_to_four_units_bldgs_rep\",\n",
    "            \"Units.6\": \"three_to_four_units_units_rep\",\n",
    "            \"Value.6\": \"three_to_four_units_value_rep\",\n",
    "            \"Bldgs.6\": \"five_or_more_units_bldgs_rep\",\n",
    "            \"Units.7\": \"five_or_more_units_units_rep\",\n",
    "            \"Value.7\": \"five_or_more_units_value_rep\",\n",
    "        },\n",
    "        inplace=True,\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def acquire_building_permits():\n",
    "    \"\"\"\n",
    "    This function conditonally acquires the building permit survey data from the US Census Bureau from the Cen\n",
    "    \"\"\"\n",
    "    \n",
    "    # conditional\n",
    "    if path.exists(\"building_permits.csv\"):\n",
    "        \n",
    "        # read csv\n",
    "        df = pd.read_csv(\"building_permits.csv\", index_col=0)\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        # create original df with 2019 data\n",
    "        df = pd.read_csv(\"https://www2.census.gov/econ/bps/Metro/ma2019a.txt\", sep=\",\", header=1)\n",
    "\n",
    "        # rename columns\n",
    "        rename_building_permit_columns(df)\n",
    "\n",
    "        for i in range(1980, 2019):\n",
    "\n",
    "            # read the txt file at url where i is the year in range\n",
    "            year_df = pd.read_csv(\n",
    "                f\"https://www2.census.gov/econ/bps/Metro/ma{i}a.txt\",\n",
    "                sep=\",\",\n",
    "                header=1,\n",
    "                names=df.columns.tolist(),\n",
    "            )\n",
    "            \n",
    "            # append data to global df variable\n",
    "            df = df.append(year_df, ignore_index=True)\n",
    "\n",
    "        # make moncov into bool so that the null observations of this feature are not considered in the dropna below\n",
    "        df[\"moncov\"] = np.where(df.moncov == \"C\", 1, 0)\n",
    "\n",
    "        # dropna inplace\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # chop off the succeding two digits after the year for survey_date\n",
    "        df[\"survey_date\"] = df.survey_date.astype(str).apply(lambda x: re.sub(r\"\\d\\d$\", \"\", x))\n",
    "        \n",
    "        # add a preceding \"19\" to any years where the length of the observation is 2 (e.i., \"80\"-\"97\")\n",
    "        df[\"survey_date\"] = df.survey_date.apply(lambda x: \"19\" + x if len(x) == 2 else x)\n",
    "        \n",
    "        # turn survey_date back into an int\n",
    "        df[\"survey_date\"] = df.survey_date.astype(int)\n",
    "        \n",
    "        # turn moncov back into a bool\n",
    "        df[\"moncov\"] = df.moncov.astype(bool)\n",
    "        \n",
    "        # sort values by survey_date\n",
    "        df.sort_values(by=[\"survey_date\"], ascending=False, inplace=True)\n",
    "\n",
    "        # reset index inplace\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "        # drop former index inplace\n",
    "        df.drop(columns=[\"index\"], inplace=True)\n",
    "        \n",
    "        # write df to disk as csv\n",
    "        df.to_csv(\"building_permits.csv\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire & Prepare\n",
    "\n",
    "All functions are called in `wrangle.py` file.  The data comes from the Department of Commerce website [click here](https://www.census.gov/construction/bps/msaannual.html).  \n",
    "\n",
    "## Functions\n",
    "- `acquire_building_permits` - acquires 23 years of \n",
    "- `rename_building_permit_columns` - renames columns from abreviation to reflect the data dictionary terms\n",
    "- `wrangle_building_permits` -  edit functions to make mother function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare\n",
    "All functions are called in `wrangle.py` file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire_building_permits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'This data frame is {df.shape[0]} rows and {df.shape[1]} columns.')\n",
    "print ('Currently, each observation is one mortgage, which will change for modelling')\n",
    "print()\n",
    "print()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fiscal_year_of_firm_commitment_activity.hist(color='green', bins=15)\n",
    "plt.title('Histogram of Mortgages by Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('project_city').final_mortgage_amount.count().nlargest(20).plot.barh(color='magenta')\n",
    "plt.title('Number of Mortgages by City')\n",
    "plt.ylabel(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(df.final_mortgage_amount)\n",
    "plt.title(\"Outliers Skewing Mortgage amount Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff = df.final_mortgage_amount.mean()-df.final_mortgage_amount.median()\n",
    "print(f'The difference between the mean and the median is {round(mean_diff)}')\n",
    "\n",
    "df.final_mortgage_amount.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In order to get our data into a useable format for modeling we decided to group the data by city and state for each fiscal year to get unique observations. Below is a brief summary of the functions found in the `preprocessing.py` script which help to restructure our data into a useable format:\n",
    "\n",
    "- `get_model_df`: This function wrangles the original data, groups the data using the city, state, and fiscal year features, and aggregates the mortgage data appropriately.\n",
    "- `calculate_city_state_vol_delta`: This function creates the growth rate for each unique city + state + year observation using total mortgage volume.\n",
    "- `calculate_city_state_qty_delta`: This function creates the growth rate for each unique city + state + year observation using the quantity of mortgages.\n",
    "- `calculate_evolution_index`: This function calculates the evolution index using the market volume delta feature created within using the market volume feature. The evolution index is a measure which expresses the growth of a unique city + state + year observation relative to the the overall market growth rate for the whole year.\n",
    "- `add_new_features`: This function calls `calculate_city_state_vol_delta`, `calculate_city_state_qty_delta`, and `calculate_evolution_index` to add new features to the modeling DataFrame.\n",
    "- `train_validate_test_data`: This function slipts our data into train, validate, and test for modeling.\n",
    "- `prep_data_for_modeling`: \n",
    "- `labeling_future_data`:  this function creates a label for the data based on whether the quantity of mortgages or volume of mortgages is an outlier.  If it is an outlier the label is `True` in the column `should_enter`. This column becomes the target variable in the modeling dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pr.get_model_df()\n",
    "print(f\"\"\"Our modeling DataFrame contains {model_df.shape[0]:,} observations & {model_df.shape[1]} features\"\"\")\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pr.add_new_features(model_df)\n",
    "print(f\"\"\"Our modeling DataFrame now contains {model_df.shape[0]:,} observations & {model_df.shape[1]} features\"\"\")\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pr.labeling_future_data(model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pr.filter_top_cities(model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "sns.boxplot(model_df.label_quantity_of_mortgages_pop_2y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "sns.boxplot(model_df.label_total_mortgage_volume_pop_2y, color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='label_total_mortgage_volume_pop_2y', y='label_quantity_of_mortgages_pop_2y', data=model_df, hue='should_enter')\n",
    "plt.title(\"Scatterplot Visualizing Markets to Enter\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "We will be using classification algorithms to predict what markets will be hot as of 2020/2021. This will help us create recommendations for the future, so that we know what market's will be worth investing resources and labor in, and what martek's are worth ignoring.\n",
    "\n",
    "We will be likely using the following features for modeling:\n",
    "\n",
    "```python\n",
    "features_for_modeling = [\"quantity_of_mortgages_pop\", \"city_state_qty_delta_pop\", \"ei\", \"median_mortgage_amount_pop\"]\n",
    "```\n",
    "\n",
    "Our target variable (the variable we are trying to predict, will be:\n",
    "\n",
    "```python\n",
    "label_feature = \"should_enter\"\n",
    "```\n",
    "\n",
    "In this case, our positive case will be `should_enter_market`. \n",
    "\n",
    "\n",
    "When looking at our confusion matrix, and all of it's possible outcomes, it would likely look as follows:\n",
    "\n",
    "| Matrix | Actual Positive | Actual Negative |\n",
    "|--------|-----------------|-----------------|\n",
    "| Predicted Positive | `enter_market` | predicted `not_enter_market`, but really it was a hot market and a missed opportunity | \n",
    "| Predicted Negative | predicted `enter_market`, but really it was a cold market, and not worth investing | `not_enter_market`\n",
    "\n",
    "\n",
    "Traditionally, for a project like this one, we would have focus on reducing the number of `False_Positives`, because it would be far more expensive to the stakeholder if we predicted a city was going to be hot, they spend time and money, and their investment is not returned. However, because TestFit's business strategy and software deployment are all done online, with very little investment needed for traveling. This means that actually investing in a city is not costly at all. As such, we will optimize our models to reduce the number of `False_Negtives`, because we want to make sure we are not missing any potential markets that can be considered `hot markets` in 2020 and 2021.\n",
    "\n",
    "Given that we have a low number of `positive` labels in our data, we will have to do something called **Oversampling**. This is a practice use in the field to basically help the predictive model by calling attention to the postiive labels and their patterns. We will create duplicate positive values, so that the model becomes more effective at predicting these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "We want to get the data from zero, as to reduce the risk of any information spillage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_for_modeling = [\"quantity_of_mortgages_pop\", \"city_state_qty_delta_pop\", \"ei\", \"median_mortgage_amount_pop\"]\n",
    "label_feature = \"should_enter\"\n",
    "train_scaled, validate_scaled, test_scaled, y_train, y_validate, y_test = preprocessing_main_function(features_for_modeling, label_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the data will look like before modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_for_modeling = [\"quantity_of_mortgages_pop\", \"city_state_qty_delta_pop\", \"ei\", \"median_mortgage_amount_pop\"]\n",
    "label_feature = \"should_enter\"\n",
    "train_scaled, validate_scaled, test_scaled, y_train, y_validate, y_test = preprocessing_main_function(features_for_modeling, label_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame({\"actual\": y_train, \"baseline\": y_train.mode()[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, 20):\n",
    "    clf, y_pred = model.run_clf(train_scaled, y_train, i)\n",
    "    score = clf.score(train_scaled, y_train)\n",
    "    validate_score = clf.score(validate_scaled, y_validate)\n",
    "    _, _, report = model.accuracy_report(clf, y_pred, y_train)\n",
    "    recall_score = report[\"True\"].recall\n",
    "    print(f\"Max_depth = {i}, accuracy_score = {score:.2f}. validate_score = {validate_score:.2f}, recall = {recall_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf, y_pred = model.run_clf(train_scaled, y_train, 4)\n",
    "predictions[\"decision_tree\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_score, matrix, report = model.accuracy_report(clf, y_pred, y_train)\n",
    "print(accuracy_score)\n",
    "print(matrix)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = clf.feature_importances_\n",
    "# We want to check that the coef array has the same number of items as there are features in our X_train dataframe.\n",
    "assert(len(coef) == train_scaled.shape[1])\n",
    "coef = clf.feature_importances_\n",
    "columns = train_scaled.columns\n",
    "df = pd.DataFrame({\"feature\": columns,\n",
    "                   \"feature_importance\": coef,\n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=\"feature_importance\", ascending=False)\n",
    "sns.barplot(data=df, x=\"feature_importance\", y=\"feature\", palette=\"Blues_d\")\n",
    "plt.title(\"What are the most influencial features?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, it seems that when it comes to decision tree, the `evolution_index` is actually the most indicative feature, along side the change in number of mortgage's approved. The total `quantity_of_mortgages_pop` doesn't seem to be as influencial in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_for_modeling = [\"quantity_of_mortgages_pop\", \"city_state_qty_delta_pop\", \"ei\", \"median_mortgage_amount_pop\"]\n",
    "label_feature = \"should_enter\"\n",
    "train_scaled, validate_scaled, test_scaled, y_train, y_validate, y_test = preprocessing_main_function(features_for_modeling, label_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(1, 20):\n",
    "    rf, y_pred = model.run_rf(train_scaled, y_train, 1, i)\n",
    "    score = rf.score(train_scaled, y_train)\n",
    "    validate_score = rf.score(validate_scaled, y_validate)\n",
    "    _, _, report = model.accuracy_report(clf, y_pred, y_train)\n",
    "    recall_score = report[\"True\"].recall\n",
    "    print(f\"Max_depth = {i}, accuracy_score = {score:.2f}. validate_score = {validate_score:.2f}, recall = {recall_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf, y_pred = model.run_rf(train_scaled, y_train, 1, 3)\n",
    "predictions[\"random_forest\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_score, matrix, report = model.accuracy_report(rf, y_pred, y_train)\n",
    "print(accuracy_score)\n",
    "print(matrix)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = rf.feature_importances_\n",
    "columns = train_scaled.columns\n",
    "df = pd.DataFrame({\"feature\": columns,\n",
    "                   \"feature_importance\": coef,\n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=\"feature_importance\", ascending=False)\n",
    "sns.barplot(data=df, x=\"feature_importance\", y=\"feature\", palette=\"Blues_d\")\n",
    "plt.title(\"What are the most influencial features?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, for the random_forest model, the delta of the number of loans approved by city where the most important or influencial indicator of whether a city would be `a hot martket` or not. The evolution index was the second most influencial feature. Again, the total `quantity_of_morgages_pop` was the least influencial feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_for_modeling = [\"quantity_of_mortgages_pop\", \"city_state_qty_delta_pop\", \"ei\", \"median_mortgage_amount_pop\"]\n",
    "label_feature = \"should_enter\"\n",
    "train_scaled, validate_scaled, test_scaled, y_train, y_validate, y_test = preprocessing_main_function(features_for_modeling, label_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 20):\n",
    "    knn, y_pred = model.run_knn(train_scaled, y_train, i)\n",
    "    score = knn.score(train_scaled, y_train)\n",
    "    validate_score = knn.score(validate_scaled, y_validate)\n",
    "    _, _, report = model.accuracy_report(clf, y_pred, y_train)\n",
    "    recall_score = report[\"True\"].recall\n",
    "    print(f\"Max_depth = {i}, accuracy_score = {score:.2f}. validate_score = {validate_score:.2f}, recall = {recall_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn, y_pred = model.run_knn(train_scaled, y_train, 2)\n",
    "predictions[\"knn\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_score, matrix, report = model.accuracy_report(knn, y_pred, y_train)\n",
    "print(accuracy_score)\n",
    "print(matrix)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How do the different models compare on accuracy?\n",
    "print(\"Accuracy Scores\")\n",
    "print(\"---------------\")\n",
    "for i in range(predictions.shape[1]):\n",
    "    report = model.create_report(predictions.actual, predictions.iloc[:,i])\n",
    "    print(f'{predictions.columns[i].title()} = {report.accuracy[0]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do the different models compare on recall?\n",
    "print(\"Recall Scores\")\n",
    "print(\"---------------\")\n",
    "for i in range(predictions.shape[1]):\n",
    "    report = model.create_report(predictions.actual, predictions.iloc[:,i])\n",
    "    print(f'{predictions.columns[i].title()} = {report[\"True\"].loc[\"recall\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do the different models compare on recall?\n",
    "print(\"Precision Scores\")\n",
    "print(\"---------------\")\n",
    "for i in range(predictions.shape[1]):\n",
    "    report = model.create_report(predictions.actual, predictions.iloc[:,i])\n",
    "    print(f'{predictions.columns[i].title()} = {report[\"True\"].loc[\"precision\"]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "Overall, we see that because we have optimized for *recall*, the accuracy scores are a bit lower than expected. However, our recall scores are really good. We will choose the KNN model as the most effective model, given that it consistently achieved the best scores (for accuracy, recall and precision). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf, y_pred = model.run_rf(train_scaled, y_train, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score, matrix, report = model.accuracy_report(rf, y_pred, y_test)\n",
    "print(accuracy_score)\n",
    "print(matrix)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn, y_pred = model.run_knn(train_scaled, y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_score, matrix, report = model.accuracy_report(knn, y_pred, y_test)\n",
    "print(accuracy_score)\n",
    "print(matrix)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pr.get_model_df()\n",
    "df = pr.add_new_features(df)\n",
    "df = pr.filter_top_cities(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_for_predicting = [\"quantity_of_mortgages_pop\", \"city_state_qty_delta_pop\", \"ei\", \"median_mortgage_amount_pop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions = df[(df.year == 2020) | (df.year == 2019)].groupby(\"city_state\")[features_for_predicting].mean()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function used to updated the scaled arrays and transform them into usable dataframes\n",
    "def return_values_prediction(scaler, df):\n",
    "    train_scaled = pd.DataFrame(scaler.transform(df), columns=df.columns.values).set_index([df.index.values])\n",
    "    return scaler, train_scaled\n",
    "\n",
    "# Linear scaler\n",
    "def min_max_scaler_prediction(df):\n",
    "    scaler = MinMaxScaler().fit(df)\n",
    "    scaler, df_scaled = return_values_prediction(scaler, df)\n",
    "    return scaler, df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler, predictions_scaled = min_max_scaler_prediction(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"label\"] = rf.predict(predictions_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = predictions.reset_index().city_state.str.split(\"_\", n=1, expand=True)[0]\n",
    "\n",
    "state = predictions.reset_index().city_state.str.split(\"_\", n=1, expand=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"city\"] = city\n",
    "\n",
    "predictions[\"state\"] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[predictions.label == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv(\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "ax = sns.barplot(data=predictions, x=\"city\", y=\"ei\", hue=\"label\")\n",
    "plt.title(\"What markets will look like in 2021, based on evolution index\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"City\")\n",
    "plt.ylabel(\"Evolution Index (%)\")\n",
    "new_labels = ['Markets to not enter', 'Markets to enter']\n",
    "h, l = ax.get_legend_handles_labels()\n",
    "ax.legend(h, new_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
